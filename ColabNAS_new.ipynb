{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7pJZhyyHImS"
      },
      "source": [
        "#Before starting:\n",
        "* This notebook is meant to run on Google Colaboratory;\n",
        "* It requires you to download a third party software, otherwise it will not run. (I know it's tedious but I don't own the program and so I can't share it with you);\n",
        "* I suggest to use an environment with a GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoivgyMxAx73"
      },
      "source": [
        "##Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHlEjP0TA53q"
      },
      "source": [
        "###Install 'tensorflow-model-optimization' for Quantization Aware Training (QAT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R8PtDnQ_Dat",
        "outputId": "de25f867-cfea-40f9-ffea-1e9ad505e132"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow ai-edge-litert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7biWvQrBB2V"
      },
      "source": [
        "###Get 'stm32tflm' software"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMu6x4Fi_bte"
      },
      "source": [
        "* Download 'X-CUBE-AI-Linux' package from https://www.st.com/en/embedded-software/x-cube-ai.html;\n",
        "* Extract the 'stm32tflm' executable from the downloaded package;\n",
        "* Put it in the folder you're working (usually '/content/' for Google Colaboratory)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPzOk0TMAg4u"
      },
      "source": [
        "Enable the execution of the 'stm32tflm' program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImbM8NNWARqg"
      },
      "outputs": [],
      "source": [
        "!chmod +x stm32tflm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tpM-Fp0Evos"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import datetime\n",
        "import shutil\n",
        "import glob\n",
        "import re\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ColabNAS code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r2J-EoGBdYw"
      },
      "outputs": [],
      "source": [
        "class ColabNAS :\n",
        "    architecture_name = 'resulting_architecture'\n",
        "    def __init__(self, max_RAM, max_Flash, max_MACC, path_to_training_set, val_split, cache=False, input_shape=(50,50,3), save_path='.', path_to_stm32tflm='/content/stm32tflm') :\n",
        "        self.learning_rate = 1e-3\n",
        "        self.batch_size = 128\n",
        "        self.epochs = 100 #minimum 2\n",
        "\n",
        "        self.max_MACC = max_MACC\n",
        "        self.max_Flash = max_Flash\n",
        "        self.max_RAM = max_RAM\n",
        "        self.path_to_training_set = path_to_training_set\n",
        "        self.num_classes = len(next(os.walk(path_to_training_set))[1])\n",
        "        self.val_split = val_split\n",
        "        self.cache = cache\n",
        "        self.input_shape = input_shape\n",
        "        self.save_path = Path(save_path)\n",
        "\n",
        "        self.path_to_trained_models = self.save_path / \"trained_models\"\n",
        "        self.path_to_trained_models.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.path_to_stm32tflm = Path(path_to_stm32tflm)\n",
        "\n",
        "        self.load_training_set()\n",
        "\n",
        "    def load_training_set(self):\n",
        "        if 3 == self.input_shape[2] :\n",
        "            color_mode = 'rgb'\n",
        "        elif 1 == self.input_shape[2] :\n",
        "            color_mode = 'grayscale'\n",
        "\n",
        "        train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "            directory= self.path_to_training_set,\n",
        "            labels='inferred',\n",
        "            label_mode='categorical',\n",
        "            color_mode=color_mode,\n",
        "            batch_size=self.batch_size,\n",
        "            image_size=self.input_shape[0:2],\n",
        "            shuffle=True,\n",
        "            seed=11,\n",
        "            validation_split=self.val_split,\n",
        "            subset='training'\n",
        "        )\n",
        "\n",
        "        validation_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "            directory= self.path_to_training_set,\n",
        "            labels='inferred',\n",
        "            label_mode='categorical',\n",
        "            color_mode=color_mode,\n",
        "            batch_size=self.batch_size,\n",
        "            image_size=self.input_shape[0:2],\n",
        "            shuffle=True,\n",
        "            seed=11,\n",
        "            validation_split=self.val_split,\n",
        "            subset='validation'\n",
        "        )\n",
        "\n",
        "        data_augmentation = tf.keras.Sequential([\n",
        "            tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "            tf.keras.layers.RandomRotation(0.2, fill_mode='constant', interpolation='bilinear'),\n",
        "            #tf.keras.layers.Rescaling(1./255)\n",
        "            ])\n",
        "\n",
        "        if self.cache :\n",
        "            self.train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "            self.validation_ds = validation_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        else :\n",
        "            self.train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "            self.validation_ds = validation_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    def get_data(self):\n",
        "        return self.train_ds, self.validation_ds\n",
        "\n",
        "    def quantize_model_uint8(self, model_name) :\n",
        "        def representative_dataset():\n",
        "            for data in self.train_ds.rebatch(1).take(150) :\n",
        "                yield [tf.dtypes.cast(data[0], tf.float32)]\n",
        "\n",
        "        model = tf.keras.models.load_model(self.path_to_trained_models / f\"{model_name}.h5\")\n",
        "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "        converter.representative_dataset = representative_dataset\n",
        "        converter.target_spec.supported_types = [tf.int8]\n",
        "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "        converter.inference_input_type = tf.uint8\n",
        "        converter.inference_output_type = tf.uint8\n",
        "        tflite_quant_model = converter.convert()\n",
        "\n",
        "        with open(self.path_to_trained_models / f\"{model_name}.tflite\", 'wb') as f:\n",
        "            f.write(tflite_quant_model)\n",
        "\n",
        "        (self.path_to_trained_models / f\"{model_name}.h5\").unlink()\n",
        "\n",
        "    def evaluate_flash_and_peak_RAM_occupancy(self, model_name) :\n",
        "        #quantize model to evaluate its peak RAM occupancy and its Flash occupancy\n",
        "        self.quantize_model_uint8(model_name)\n",
        "\n",
        "        #evaluate its peak RAM occupancy and its Flash occupancy using STMicroelectronics' X-CUBE-AI\n",
        "        proc = subprocess.Popen([self.path_to_stm32tflm, self.path_to_trained_models / f\"{model_name}.tflite\"], stdout=subprocess.PIPE)\n",
        "        try:\n",
        "            outs, errs = proc.communicate(timeout=15)\n",
        "            Flash, RAM = re.findall(r'\\d+', str(outs))\n",
        "        except subprocess.TimeoutExpired:\n",
        "            proc.kill()\n",
        "            outs, errs = proc.communicate()\n",
        "            print(\"stm32tflm error\")\n",
        "            exit()\n",
        "\n",
        "        return int(Flash), int(RAM)\n",
        "\n",
        "    def evaluate_model(self, model, MACC, number_of_cells_limited, model_name) :\n",
        "        print(f\"\\n{model_name}\\n\")\n",
        "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "            str(self.path_to_trained_models / f\"{model_name}.h5\"), monitor='val_accuracy',\n",
        "            verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
        "        #One epoch of training must be done before quantization, which is needed to evaluate RAM and Flash occupancy\n",
        "        model.fit(self.train_ds, epochs=1, validation_data=self.validation_ds, validation_freq=1)\n",
        "        model.save(self.path_to_trained_models / f\"{model_name}.h5\")\n",
        "        Flash, RAM = self.evaluate_flash_and_peak_RAM_occupancy(model_name)\n",
        "        print(f\"\\nRAM: {RAM},\\t Flash: {Flash},\\t MACC: {MACC}\\n\")\n",
        "        if MACC <= self.max_MACC and Flash <= self.max_Flash and RAM <= self.max_RAM and not number_of_cells_limited :\n",
        "            hist = model.fit(self.train_ds, epochs=self.epochs - 1, validation_data=self.validation_ds, validation_freq=1, callbacks=[checkpoint])\n",
        "            self.quantize_model_uint8(model_name)\n",
        "        return {'RAM': RAM if RAM <= self.max_RAM else \"Outside the upper bound\",\n",
        "                'Flash': Flash if Flash <= self.max_Flash else \"Outside the upper bound\",\n",
        "                'MACC': MACC if MACC <= self.max_MACC else \"Outside the upper bound\",\n",
        "                'max_val_acc':\n",
        "                np.around(np.amax(hist.history['val_accuracy']), decimals=3)\n",
        "                if 'hist' in locals() else -3}\n",
        "\n",
        "    def search(self, NAS):\n",
        "      nas = NAS(\n",
        "        evaluate_model_fnc = self.evaluate_model, \n",
        "        input_shape = self.input_shape, \n",
        "        num_classes = self.num_classes, \n",
        "        learning_rate = self.learning_rate\n",
        "        )\n",
        "      resulting_architecture, take_time = nas.search()\n",
        "\n",
        "      if (resulting_architecture['max_val_acc'] > 0) :\n",
        "            resulting_architecture_name = f\"k_{resulting_architecture['k']}_c_{resulting_architecture['c']}.tflite\"\n",
        "            self.path_to_resulting_architecture = self.save_path / f\"resulting_architecture_{resulting_architecture_name}\"\n",
        "            (self.path_to_trained_models / f\"{resulting_architecture_name}\").rename(self.path_to_resulting_architecture)\n",
        "            shutil.rmtree(self.path_to_trained_models)\n",
        "            print(f\"\\nResulting architecture: {resulting_architecture}\\n\")\n",
        "      else :\n",
        "          print(f\"\\nNo feasible architecture found\\n\")\n",
        "      print(f\"Elapsed time (search): {take_time}\\n\")\n",
        "\n",
        "      return self.path_to_resulting_architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ai_edge_litert.interpreter import Interpreter # Changed class name to Interpreter\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def test_litert_model(path_to_resulting_model, test_ds):\n",
        "    # Initialize the LiteRT Interpreter\n",
        "    interpreter = Interpreter(model_path=str(path_to_resulting_model))\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    output_details = interpreter.get_output_details()[0]\n",
        "    input_details = interpreter.get_input_details()[0]\n",
        "    input_dtype = input_details['dtype']\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images_batch, labels_batch in test_ds:\n",
        "        for image, label in zip(images_batch, labels_batch):\n",
        "            # 1. Handle Quantization Rescaling\n",
        "            if input_dtype == np.uint8 or input_dtype == tf.uint8:\n",
        "                scale, zero_point = input_details[\"quantization\"]\n",
        "                # Convert to integer via the formula: q = (f / scale) + zero_point\n",
        "                image = (image / scale) + zero_point\n",
        "\n",
        "            # 2. Cast and add batch dimension\n",
        "            input_data = np.expand_dims(image.numpy().astype(input_dtype), axis=0)\n",
        "\n",
        "            # 3. Inference\n",
        "            interpreter.set_tensor(input_details['index'], input_data)\n",
        "            interpreter.invoke()\n",
        "\n",
        "            # 4. Get results\n",
        "            prediction = interpreter.get_tensor(output_details['index'])\n",
        "\n",
        "            if np.argmax(label) == np.argmax(prediction):\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    print(f\"\\nLiteRT model test accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Usage\n",
        "test_litert_model(path_to_tflite_model, test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OurNAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLL1srV4EJMC"
      },
      "outputs": [],
      "source": [
        "class OurNAS():\n",
        "  architecture_name = 'resulting_architecture'\n",
        "  def __init__(self, evaluate_model_fnc, input_shape, num_classes, learning_rate, evaluate_model):\n",
        "    self.evaluate_model_fnc = evaluate_model_fnc\n",
        "    self.model_count = 0\n",
        "    self.model_name = \"\"\n",
        "    self.input_shape = input_shape\n",
        "    self.num_classes = num_classes\n",
        "    self.learning_rate = learning_rate\n",
        "    self.evaluate_model = evaluate_model\n",
        "\n",
        "  def create_model(self, k, c):\n",
        "    kernel_size = (3,3)\n",
        "    pool_size = (2,2)\n",
        "    pool_strides = (2,2)\n",
        "\n",
        "    number_of_cells_limited = False\n",
        "    number_of_mac = 0\n",
        "\n",
        "    inputs = keras.Input(shape=self.input_shape)\n",
        "\n",
        "    #convolutional base\n",
        "    n = int(k)\n",
        "    multiplier = 2\n",
        "\n",
        "    #first convolutional layer\n",
        "    c_in = self.input_shape[2]\n",
        "    x = keras.layers.Conv2D(n, kernel_size, padding='same')(inputs)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    x = keras.layers.ReLU()(x)\n",
        "\n",
        "    number_of_mac = number_of_mac + (c_in * kernel_size[0] * kernel_size[1] * x.shape[1] * x.shape[2] * x.shape[3])\n",
        "\n",
        "    #adding cells\n",
        "    for i in range(1, c + 1) :\n",
        "        if x.shape[1] <= 1 or x.shape[2] <= 1 :\n",
        "            number_of_cells_limited = True\n",
        "            break;\n",
        "        n = int(np.ceil(n * multiplier))\n",
        "        multiplier = multiplier - 2**-i\n",
        "        x = keras.layers.MaxPooling2D(pool_size=pool_size, strides=pool_strides, padding='valid')(x)\n",
        "        c_in = x.shape[3]\n",
        "        x = keras.layers.Conv2D(n, kernel_size, padding='same')(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.ReLU()(x)\n",
        "        number_of_mac = number_of_mac + (c_in * kernel_size[0] * kernel_size[1] * x.shape[1] * x.shape[2] * x.shape[3])\n",
        "\n",
        "    #classifier\n",
        "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "    input_shape = x.shape[1]\n",
        "    x = keras.layers.Dense(n)(x)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    x = keras.layers.ReLU()(x)\n",
        "    number_of_mac = number_of_mac + (input_shape * x.shape[1])\n",
        "    x = keras.layers.Dense(self.num_classes)(x)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    outputs = keras.layers.Softmax()(x)\n",
        "    number_of_mac = number_of_mac + (x.shape[1] * outputs.shape[1])\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "    model.compile(optimizer=opt,\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model, number_of_mac, number_of_cells_limited\n",
        "\n",
        "  def search(self):\n",
        "    self.model_counter = 0\n",
        "    epsilon = 0.005\n",
        "    k0 = 4\n",
        "\n",
        "    start = datetime.datetime.now()\n",
        "\n",
        "    k = k0\n",
        "    previous_architecture = self.explore_num_cells(k)\n",
        "    k = 2 * k\n",
        "    current_architecture = self.explore_num_cells(k)\n",
        "\n",
        "    if (current_architecture['max_val_acc'] > previous_architecture['max_val_acc']) :\n",
        "        previous_architecture = current_architecture\n",
        "        k = 2 * k\n",
        "        current_architecture = self.explore_num_cells(k)\n",
        "        while(current_architecture['max_val_acc'] > previous_architecture['max_val_acc'] + epsilon) :\n",
        "            previous_architecture = current_architecture\n",
        "            k = 2 * k\n",
        "            current_architecture = self.explore_num_cells(k)\n",
        "    else :\n",
        "        k = k0 / 2\n",
        "        current_architecture = self.explore_num_cells(k)\n",
        "        while(current_architecture['max_val_acc'] >= previous_architecture['max_val_acc']) :\n",
        "            previous_architecture = current_architecture\n",
        "            k = k / 2\n",
        "            current_architecture = self.explore_num_cells(k)\n",
        "\n",
        "    resulting_architecture = previous_architecture\n",
        "    end = datetime.datetime.now()\n",
        "\n",
        "    return resulting_architecture, end-start\n",
        "\n",
        "  def explore_num_cells(self, k) :\n",
        "      previous_architecture = {'k': -1, 'c': -1, 'max_val_acc': -2}\n",
        "      current_architecture = {'k': -1, 'c': -1, 'max_val_acc': -1}\n",
        "      c = -1\n",
        "      k = int(k)\n",
        "\n",
        "      while(current_architecture['max_val_acc'] > previous_architecture['max_val_acc']) :\n",
        "          previous_architecture = current_architecture\n",
        "          c = c + 1\n",
        "          self.model_counter = self.model_counter + 1\n",
        "          current_architecture = self.evaluate_model_process(k, c)\n",
        "          print(f\"\\n\\n\\n{current_architecture}\\n\\n\\n\")\n",
        "      return previous_architecture\n",
        "\n",
        "\n",
        "  def evaluate_model_process(self, k, c):\n",
        "    if k > 0 :\n",
        "      self.model_name = f\"k_{k}_c_{c}\"\n",
        "      model, MACC, number_of_cells_limited = self.create_model(k, c)\n",
        "      result_property_dict = self.evaluate_model(model, MACC, number_of_cells_limited, self.model_name)\n",
        "      result_property_dict[\"k\"] = k\n",
        "      result_property_dict[\"c\"] = c if not number_of_cells_limited else \"Not feasible\"\n",
        "      return result_property_dict\n",
        "    else :\n",
        "      return{'k': 'unfeasible', 'c': c, 'max_val_acc': -3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "data_dir = tf.keras.utils.get_file('flower_photos.tar', origin=dataset_url, extract=True)\n",
        "data_dir = Path(data_dir).with_suffix('') / \"flower_photos\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "input_shape = (50,50,3)\n",
        "\n",
        "#target: STM32L412KBU3\n",
        "#273 CoreMark, 40 kiB RAM, 128 kiB Flash\n",
        "peak_RAM_upper_bound = 40960\n",
        "Flash_upper_bound = 131072\n",
        "MACC_upper_bound = 2730000 #CoreMark * 1e4\n",
        "\n",
        "#Each dataset must comply with the following structure\n",
        "#main_directory/\n",
        "#...class_a/\n",
        "#......a_image_1.jpg\n",
        "#......a_image_2.jpg\n",
        "#...class_b/\n",
        "#......b_image_1.jpg\n",
        "#......b_image_2.jpg\n",
        "path_to_training_set = data_dir\n",
        "val_split = 0.3\n",
        "\n",
        "#whether or not to cache datasets in memory\n",
        "#if the dataset cannot fit in the main memory, the application will crash\n",
        "cache = True\n",
        "\n",
        "#where to save results\n",
        "save_path = '/content/'\n",
        "\n",
        "#to show the GPU used\n",
        "!nvidia-smi\n",
        "\n",
        "colabNAS = ColabNAS(peak_RAM_upper_bound, Flash_upper_bound, MACC_upper_bound, path_to_training_set, val_split, cache, input_shape, save_path=save_path)\n",
        "\n",
        "#search\n",
        "path_to_tflite_model = colabNAS.search(OurNAS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_, test_ds = colabNAS.get_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_litert_model(path_to_tflite_model, test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Second NAS (PSO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1. SEARCH SPACE & ENCODING ---\n",
        "# We represent an architecture as a vector: [k, c]\n",
        "# k: Initial number of filters (Continuous, then rounded to int)\n",
        "# c: Number of additional cells (Continuous, then rounded to int)\n",
        "\n",
        "class ArchitectureSearchSpace:\n",
        "    def __init__(self, k_range=(4, 128), c_range=(0, 10)):\n",
        "        self.k_min, self.k_max = k_range\n",
        "        self.c_min, self.c_max = c_range\n",
        "\n",
        "    def clamp(self, k, c):\n",
        "        \"\"\"Ensures particles stay within the defined search space.\"\"\"\n",
        "        return np.clip(k, self.k_min, self.k_max), np.clip(c, self.c_min, self.c_max)\n",
        "\n",
        "# --- 2. DECODER (Model Creator) ---\n",
        "class ModelDecoder:\n",
        "    def __init__(self):\n",
        "        self.input_shape = None\n",
        "        self.num_classes = None\n",
        "        self.learning_rate = None\n",
        "\n",
        "    def decode_and_build(self, k, c):\n",
        "        \"\"\"Transforms PSO coordinates into a Keras model + MAC count.\"\"\"\n",
        "        k, c = int(k), int(c)\n",
        "        kernel_size = (3, 3)\n",
        "        pool_size = (2, 2)\n",
        "        \n",
        "        number_of_mac = 0\n",
        "        number_of_cells_limited = False\n",
        "        \n",
        "        inputs = keras.Input(shape=self.input_shape)\n",
        "        n = k\n",
        "        multiplier = 2\n",
        "        \n",
        "        # First Layer\n",
        "        c_in = self.input_shape[2]\n",
        "        x = keras.layers.Conv2D(n, kernel_size, padding='same')(inputs)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.ReLU()(x)\n",
        "        \n",
        "        # Simplified MAC calculation for clarity\n",
        "        number_of_mac += (c_in * np.prod(kernel_size) * x.shape[1] * x.shape[2] * x.shape[3])\n",
        "\n",
        "        # Adding Cells\n",
        "        for i in range(1, c + 1):\n",
        "            if x.shape[1] <= 1 or x.shape[2] <= 1:\n",
        "                number_of_cells_limited = True\n",
        "                break\n",
        "            \n",
        "            n = int(np.ceil(n * multiplier))\n",
        "            multiplier -= 2**-i\n",
        "            x = keras.layers.MaxPooling2D(pool_size=pool_size, strides=(2,2), padding='valid')(x)\n",
        "            \n",
        "            c_in = x.shape[3]\n",
        "            x = keras.layers.Conv2D(n, kernel_size, padding='same')(x)\n",
        "            x = keras.layers.BatchNormalization()(x)\n",
        "            x = keras.layers.ReLU()(x)\n",
        "            number_of_mac += (c_in * np.prod(kernel_size) * x.shape[1] * x.shape[2] * x.shape[3])\n",
        "\n",
        "        # Classifier\n",
        "        x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "        feat_dim = x.shape[1]\n",
        "        x = keras.layers.Dense(n)(x)\n",
        "        number_of_mac += (feat_dim * n)\n",
        "        x = keras.layers.Dense(self.num_classes)(x)\n",
        "        outputs = keras.layers.Softmax()(x)\n",
        "        number_of_mac += (n * self.num_classes)\n",
        "\n",
        "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "        model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        \n",
        "        return model, number_of_mac, number_of_cells_limited\n",
        "\n",
        "# --- 3. EVALUATOR & CONSTRAINTS ---\n",
        "class NASPsoOptimizer:\n",
        "    def __init__(self, evaluate_model_fnc, input_shape, num_classes, learning_rate):\n",
        "        self.evaluate_model_fnc = evaluate_model_fnc # External fnc that returns {'max_val_acc': float}\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.model_name = \"\"\n",
        "        \n",
        "        self.setup_decoder()\n",
        "        \n",
        "    def setup_decoder(self):\n",
        "        print(type(self.decoder))\n",
        "        self.decoder.input_shape=self.input_shape\n",
        "        self.decoder.num_classes=self.num_classes\n",
        "        self.decoder.learning_rate=self.learning_rate\n",
        "\n",
        "    def search(self):\n",
        "        # Initialize particles [k, c] and velocities\n",
        "        particles = np.array([\n",
        "            [np.random.uniform(self.space.k_min, self.space.k_max), \n",
        "             np.random.uniform(self.space.c_min, self.space.c_max)] \n",
        "            for _ in range(self.n_particles)\n",
        "        ])\n",
        "        velocities = np.zeros((self.n_particles, 2))\n",
        "        \n",
        "        p_best = np.copy(particles)\n",
        "        p_best_scores = np.full(self.n_particles, -1.0)\n",
        "        \n",
        "        g_best = None\n",
        "        g_best_score = -1.0\n",
        "\n",
        "        w, c1, c2 = 0.5, 1.5, 1.5 # Hyperparameters for PSO\n",
        "\n",
        "        start = datetime.datetime.now()\n",
        "        \n",
        "        results_best = {}\n",
        "        for it in range(self.iterations):\n",
        "            print(f\"==================== iteration {it} ====================\")\n",
        "            for i in range(self.n_particles):\n",
        "                k, c = particles[i]\n",
        "                \n",
        "                # Build and Evaluate\n",
        "                model, macc, limited = self.decoder.decode_and_build(k, c)\n",
        "                \n",
        "                # Constraint Check\n",
        "                if limited:\n",
        "                    score = 0 # Penalty for invalid architectures\n",
        "                else:\n",
        "                    self.model_name = f\"k_{int(k)}_c_{int(c)}\"\n",
        "                    results = self.evaluate_model_fnc(model, macc, limited, self.model_name)\n",
        "                    score = results['max_val_acc']\n",
        "\n",
        "                # Update Personal Best\n",
        "                if score > p_best_scores[i]:\n",
        "                    p_best_scores[i] = score\n",
        "                    p_best[i] = particles[i]\n",
        "\n",
        "                # Update Global Best\n",
        "                if score > g_best_score:\n",
        "                    results_best = results\n",
        "                    g_best_score = score\n",
        "                    g_best = np.copy(particles[i])\n",
        "\n",
        "            # Update Velocities and Positions\n",
        "            if g_best is None:\n",
        "                # Option A: If no valid architecture was found, re-randomize or skip update\n",
        "                print(\"Warning: No valid architecture found in this iteration. Re-randomizing velocities...\")\n",
        "                velocities = np.random.uniform(-1, 1, size=velocities.shape)\n",
        "            else:\n",
        "                # Standard PSO Update logic\n",
        "                for i in range(self.n_particles):\n",
        "                    r1, r2 = np.random.rand(), np.random.rand()\n",
        "                    velocities[i] = (w * velocities[i] + \n",
        "                                    c1 * r1 * (p_best[i] - particles[i]) + \n",
        "                                    c2 * r2 * (g_best - particles[i])) # No longer crashes\n",
        "                    \n",
        "                    particles[i] += velocities[i]\n",
        "                    particles[i][0], particles[i][1] = self.space.clamp(particles[i][0], particles[i][1])\n",
        "\n",
        "                print(f\"Iteration {it}: Global Best Score = {g_best_score:.4f} at k={int(g_best[0])}, c={int(g_best[1])}\")\n",
        "\n",
        "        results_best[\"k\"] = int(g_best[0])\n",
        "        results_best[\"c\"] = int(g_best[1])\n",
        "        \n",
        "        end = datetime.datetime.now()\n",
        "        return results_best, end-start\n",
        "    \n",
        "    def setup(search_space, decoder,  n_particles=5, iterations=10):\n",
        "        NASPsoOptimizer.n_particles = n_particles\n",
        "        NASPsoOptimizer.iterations = iterations\n",
        "        NASPsoOptimizer.space = search_space\n",
        "        NASPsoOptimizer.decoder = decoder\n",
        "        return NASPsoOptimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_shape = (50,50,3)\n",
        "\n",
        "#target: STM32L412KBU3\n",
        "#273 CoreMark, 40 kiB RAM, 128 kiB Flash\n",
        "peak_RAM_upper_bound = 40960\n",
        "Flash_upper_bound = 131072\n",
        "MACC_upper_bound = 2730000 #CoreMark * 1e4\n",
        "\n",
        "#Each dataset must comply with the following structure\n",
        "#main_directory/\n",
        "#...class_a/\n",
        "#......a_image_1.jpg\n",
        "#......a_image_2.jpg\n",
        "#...class_b/\n",
        "#......b_image_1.jpg\n",
        "#......b_image_2.jpg\n",
        "path_to_training_set = data_dir\n",
        "val_split = 0.3\n",
        "\n",
        "#whether or not to cache datasets in memory\n",
        "#if the dataset cannot fit in the main memory, the application will crash\n",
        "cache = True\n",
        "\n",
        "#where to save results\n",
        "save_path = ''\n",
        "\n",
        "#to show the GPU used\n",
        "!nvidia-smi\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "search_space = ArchitectureSearchSpace(k_range=(2, 10), c_range=(1, 5))\n",
        "decoder = ModelDecoder()\n",
        "\n",
        "# Pass your existing evaluation logic here\n",
        "colabNAS = ColabNAS(peak_RAM_upper_bound, Flash_upper_bound, MACC_upper_bound, path_to_training_set, val_split, cache, input_shape, save_path=save_path)\n",
        "#search\n",
        "path_to_tflite_model = colabNAS.search(NASPsoOptimizer.setup(search_space, decoder))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_, test_ds = colabNAS.get_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_litert_model(path_to_tflite_model, test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDjkqn-36eob"
      },
      "source": [
        "## SEP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMbeEgXuqO2x"
      },
      "outputs": [],
      "source": [
        "class OurNAS():\n",
        "  def __init__(self, evaluate_model_fnc):\n",
        "    self.architecture_name = 'resulting_architecture'\n",
        "    self.evaluate_model_fnc = evaluate_model_fnc\n",
        "    self.model = None\n",
        "    self.model_count = 0\n",
        "\n",
        "  def create_model(self, k, c):\n",
        "    # create model\n",
        "    # return model, MACC, is_valid # is_valid is status that check if the architecture is valid or not\n",
        "    pass\n",
        "\n",
        "  def search(self):\n",
        "    # count self.model_count every iter\n",
        "    # use PSO algorithm to fine optimize model architechture and check constraint is meet (like)\n",
        "    # self.model\n",
        "    pass\n",
        "\n",
        "  def evaluate_model_process(self):\n",
        "    # call the evaluate_model_func(self.model)\n",
        "    # model, MACC, number_of_cells_limited = self.Model(k, c)\n",
        "    pass\n",
        "\n",
        "  def objective_funtion(self):\n",
        "    return self.evaluate_model_fnc(self.model) # placeholder objective function (now, it only use model accuracy) in future i will add more objective"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
