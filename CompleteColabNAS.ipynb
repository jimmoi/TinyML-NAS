{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local ColabNAS - Complete with Test Set and Visualization\n",
    "\n",
    "Using the complete flower dataset with test accuracy and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "GPU available: []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from local_colabnas import LocalColabNAS, test_tflite_model\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory: C:\\Users\\Chakkaphan\\.keras\\datasets\\flower_photos_extracted\\flower_photos\n",
      "Classes found: ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n"
     ]
    }
   ],
   "source": [
    "# Download and setup dataset\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file('flower_photos.tar', origin=dataset_url, extract=True)\n",
    "data_dir = Path(data_dir).with_suffix('')\n",
    "\n",
    "# Fix dataset structure\n",
    "actual_data_dir = data_dir / 'flower_photos'\n",
    "if actual_data_dir.exists():\n",
    "    data_dir = actual_data_dir\n",
    "\n",
    "print(f\"Dataset directory: {data_dir}\")\n",
    "print(f\"Classes found: {[d.name for d in data_dir.iterdir() if d.is_dir()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Input shape: (50, 50, 3)\n",
      "  RAM limit: 40960 bytes\n",
      "  Flash limit: 131072 bytes\n",
      "  MACC limit: 2730000\n"
     ]
    }
   ],
   "source": [
    "# Exact configuration\n",
    "input_shape = (50,50,3)\n",
    "\n",
    "#target: STM32L412KBU3\n",
    "#273 CoreMark, 40 kiB RAM, 128 kiB Flash\n",
    "peak_RAM_upper_bound = 40960\n",
    "Flash_upper_bound = 131072\n",
    "MACC_upper_bound = 2730000 #CoreMark * 1e4\n",
    "\n",
    "path_to_training_set = data_dir\n",
    "val_split = 0.3\n",
    "cache = True\n",
    "save_path = './trained_models/'\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Input shape: {input_shape}\")\n",
    "print(f\"  RAM limit: {peak_RAM_upper_bound} bytes\")\n",
    "print(f\"  Flash limit: {Flash_upper_bound} bytes\")\n",
    "print(f\"  MACC limit: {MACC_upper_bound}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Information:\n",
      "  No GPU detected - using CPU\n",
      "Initializing LocalColabNAS with 5 classes\n",
      "Found 3670 files belonging to 5 classes.\n",
      "Using 2569 files for training.\n",
      "Found 3670 files belonging to 5 classes.\n",
      "Using 1101 files for validation.\n",
      "\n",
      "ColabNAS initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# GPU check\n",
    "print(\"GPU Information:\")\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    for gpu in tf.config.list_physical_devices('GPU'):\n",
    "        print(f\"  {gpu}\")\n",
    "else:\n",
    "    print(\"  No GPU detected - using CPU\")\n",
    "\n",
    "# Initialize ColabNAS\n",
    "colabNAS = LocalColabNAS(\n",
    "    max_RAM=peak_RAM_upper_bound, \n",
    "    max_Flash=Flash_upper_bound, \n",
    "    max_MACC=MACC_upper_bound, \n",
    "    path_to_training_set=str(path_to_training_set), \n",
    "    val_split=val_split, \n",
    "    cache=cache, \n",
    "    input_shape=input_shape, \n",
    "    save_path=save_path\n",
    ")\n",
    "\n",
    "print(\"\\nColabNAS initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.2408 - loss: 1.8592 - val_accuracy: 0.2262 - val_loss: 1.5953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAM: 32092, Flash: 99224, MACC: 211165\n",
      "\n",
      "Epoch 1/99\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3575 - loss: 1.4780\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25431, saving model to trained_models\\trained_models\\k_2_c_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3578 - loss: 1.4775 - val_accuracy: 0.2543 - val_loss: 1.5842\n",
      "Epoch 2/99\n",
      "\u001b[1m78/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3758 - loss: 1.4212\n",
      "Epoch 2: val_accuracy improved from 0.25431 to 0.29791, saving model to trained_models\\trained_models\\k_2_c_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3772 - loss: 1.4191 - val_accuracy: 0.2979 - val_loss: 1.5427\n",
      "Epoch 3/99\n",
      "\u001b[1m72/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4072 - loss: 1.3739\n",
      "Epoch 3: val_accuracy improved from 0.29791 to 0.34968, saving model to trained_models\\trained_models\\k_2_c_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4109 - loss: 1.3683 - val_accuracy: 0.3497 - val_loss: 1.4644\n",
      "Epoch 4/99\n",
      "\u001b[1m80/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4307 - loss: 1.3218\n",
      "Epoch 4: val_accuracy improved from 0.34968 to 0.40599, saving model to trained_models\\trained_models\\k_2_c_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4317 - loss: 1.3209 - val_accuracy: 0.4060 - val_loss: 1.3749\n",
      "Epoch 5/99\n",
      "\u001b[1m77/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4574 - loss: 1.2863\n",
      "Epoch 5: val_accuracy improved from 0.40599 to 0.44233, saving model to trained_models\\trained_models\\k_2_c_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4597 - loss: 1.2841 - val_accuracy: 0.4423 - val_loss: 1.3426\n",
      "Epoch 6/99\n",
      "\u001b[1m75/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4714 - loss: 1.2569\n",
      "Epoch 6: val_accuracy did not improve from 0.44233\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4746 - loss: 1.2538 - val_accuracy: 0.4405 - val_loss: 1.3282\n",
      "Epoch 7/99\n",
      "\u001b[1m73/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4954 - loss: 1.2278\n",
      "Epoch 7: val_accuracy did not improve from 0.44233\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4985 - loss: 1.2238 - val_accuracy: 0.4305 - val_loss: 1.3360\n",
      "Epoch 8/99\n",
      "\u001b[1m74/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5004 - loss: 1.2006\n",
      "Epoch 8: val_accuracy did not improve from 0.44233\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5031 - loss: 1.1973 - val_accuracy: 0.3842 - val_loss: 1.3787\n",
      "Epoch 9/99\n",
      "\u001b[1m78/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5134 - loss: 1.1730\n",
      "Epoch 9: val_accuracy did not improve from 0.44233\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5147 - loss: 1.1715 - val_accuracy: 0.3869 - val_loss: 1.3742\n",
      "Epoch 10/99\n",
      "\u001b[1m71/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5288 - loss: 1.1510\n",
      "Epoch 10: val_accuracy did not improve from 0.44233\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5321 - loss: 1.1469 - val_accuracy: 0.3642 - val_loss: 1.4197\n",
      "Epoch 11/99\n",
      "\u001b[1m75/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5416 - loss: 1.1288\n",
      "Epoch 11: val_accuracy did not improve from 0.44233\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5433 - loss: 1.1263 - val_accuracy: 0.3415 - val_loss: 1.4803\n",
      "Epoch 12/99\n",
      "\u001b[1m78/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5431 - loss: 1.1111\n",
      "Epoch 12: val_accuracy did not improve from 0.44233\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5441 - loss: 1.1097 - val_accuracy: 0.3351 - val_loss: 1.5056\n",
      "Epoch 13/99\n",
      "\u001b[1m74/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5506 - loss: 1.0962\n",
      "Epoch 13: val_accuracy did not improve from 0.44233\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5525 - loss: 1.0933 - val_accuracy: 0.3706 - val_loss: 1.4295\n",
      "Epoch 14/99\n",
      "\u001b[1m72/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5647 - loss: 1.0795\n",
      "Epoch 14: val_accuracy did not improve from 0.44233\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5664 - loss: 1.0761 - val_accuracy: 0.3915 - val_loss: 1.3929\n",
      "Epoch 15/99\n",
      "\u001b[1m74/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5729 - loss: 1.0635\n",
      "Epoch 15: val_accuracy did not improve from 0.44233\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5747 - loss: 1.0610 - val_accuracy: 0.3906 - val_loss: 1.4126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\CHAKKA~1\\AppData\\Local\\Temp\\tmpaob070xw\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\CHAKKA~1\\AppData\\Local\\Temp\\tmpaob070xw\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\CHAKKA~1\\AppData\\Local\\Temp\\tmpaob070xw'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 50, 3), dtype=tf.float32, name='input_layer_9')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 5), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2362755239072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755250304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755268672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755269728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755251184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755252064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755272368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755272016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755282368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755283072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755281840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755282016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755298048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755297872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755299808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755300512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755299280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755299456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755315488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755315312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755342400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755342576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755316720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755316896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755344864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755344688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755367152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755368208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755346096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2362755366976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chakkaphan\\anaconda3\\envs\\tinyml\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:863: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "{'k': 2, 'c': 2, 'RAM': 32092, 'Flash': 99224, 'MACC': 211165, 'max_val_acc': 0.442}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'trained_models\\\\trained_models\\\\k_4_c_1.tflite' -> 'trained_models\\\\resulting_architecture_k_4_c_1.tflite'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Search for optimal architecture\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting search...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m path_to_tflite_model \u001b[38;5;241m=\u001b[39m \u001b[43mcolabNAS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path_to_tflite_model:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSearch completed! Model saved at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_to_tflite_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32ms:\\TinyML\\Workspace\\local_colabnas.py:309\u001b[0m, in \u001b[0;36mLocalColabNAS.search\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    307\u001b[0m source_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_to_trained_models \u001b[38;5;241m/\u001b[39m resulting_architecture_name\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m--> 309\u001b[0m     \u001b[43msource_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_to_resulting_architecture\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResulting architecture: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresulting_architecture\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_to_resulting_architecture\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Chakkaphan\\anaconda3\\envs\\tinyml\\lib\\pathlib.py:1310\u001b[0m, in \u001b[0;36mPath.rename\u001b[1;34m(self, target)\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrename\u001b[39m(\u001b[38;5;28mself\u001b[39m, target):\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;124;03m    Rename this path to the target path.\u001b[39;00m\n\u001b[0;32m   1303\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;124;03m    Returns the new Path instance pointing to the target path.\u001b[39;00m\n\u001b[0;32m   1309\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1310\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(target)\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'trained_models\\\\trained_models\\\\k_4_c_1.tflite' -> 'trained_models\\\\resulting_architecture_k_4_c_1.tflite'"
     ]
    }
   ],
   "source": [
    "# Search for optimal architecture\n",
    "print(\"Starting search...\")\n",
    "path_to_tflite_model = colabNAS.search()\n",
    "\n",
    "if path_to_tflite_model:\n",
    "    print(f\"\\nSearch completed! Model saved at: {path_to_tflite_model}\")\n",
    "else:\n",
    "    print(\"\\nNo feasible architecture found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_to_tflite_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create separate test set and test model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mpath_to_tflite_model\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m Path(path_to_tflite_model)\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Create test dataset (separate from train/val)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     test_ds \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[0;32m      5\u001b[0m         directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(data_dir),\n\u001b[0;32m      6\u001b[0m         labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minferred\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m         subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     15\u001b[0m     )\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'path_to_tflite_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Create separate test set and test model\n",
    "if path_to_tflite_model and Path(path_to_tflite_model).exists():\n",
    "    # Create test dataset (separate from train/val)\n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=str(data_dir),\n",
    "        labels='inferred',\n",
    "        label_mode='categorical',\n",
    "        color_mode='rgb',\n",
    "        batch_size=1,\n",
    "        image_size=input_shape[0:2],\n",
    "        shuffle=True,\n",
    "        seed=123,  # Different seed for test set\n",
    "        validation_split=0.9,  # Use only 10% for testing\n",
    "        subset='validation'\n",
    "    )\n",
    "    \n",
    "    print(\"Testing model...\")\n",
    "    accuracy = test_tflite_model(path_to_tflite_model, test_ds)\n",
    "    \n",
    "    model_size = Path(path_to_tflite_model).stat().st_size\n",
    "    print(f\"Model size: {model_size/1024:.2f} KB\")\n",
    "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "else:\n",
    "    print(\"No model to test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "def visualize_predictions(model_path, test_dataset, num_samples=6):\n",
    "    interpreter = tf.lite.Interpreter(str(model_path))\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "    \n",
    "    class_names = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    sample_count = 0\n",
    "    for batch in test_dataset.take(num_samples):\n",
    "        image, label = batch[0], batch[1]\n",
    "        true_label = label.numpy().argmax()\n",
    "        \n",
    "        # Prepare input\n",
    "        if input_details['dtype'] == tf.uint8:\n",
    "            scale, zero_point = input_details['quantization']\n",
    "            image_input = image / scale + zero_point\n",
    "            image_input = tf.cast(image_input, tf.uint8)\n",
    "        else:\n",
    "            image_input = tf.cast(image, input_details['dtype'])\n",
    "        \n",
    "        # Predict\n",
    "        interpreter.set_tensor(input_details['index'], image_input)\n",
    "        interpreter.invoke()\n",
    "        prediction = interpreter.get_tensor(output_details['index'])\n",
    "        predicted_label = prediction.argmax()\n",
    "        confidence = prediction.max()\n",
    "        \n",
    "        # Plot\n",
    "        plt.subplot(2, 3, sample_count + 1)\n",
    "        plt.imshow(image[0].numpy().astype(np.uint8))\n",
    "        color = 'green' if predicted_label == true_label else 'red'\n",
    "        plt.title(f'True: {class_names[true_label]}\\nPred: {class_names[predicted_label]}\\nConf: {confidence:.2f}', color=color)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        sample_count += 1\n",
    "        if sample_count >= num_samples:\n",
    "            break\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions\n",
    "if path_to_tflite_model and Path(path_to_tflite_model).exists():\n",
    "    print(\"\\nVisualizing predictions:\")\n",
    "    visualize_predictions(path_to_tflite_model, test_ds)\n",
    "else:\n",
    "    print(\"No model available for visualization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
