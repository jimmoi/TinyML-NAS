{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Improved ColabNAS with Block Variants\n",
        "This notebook implements an enhanced version of ColabNAS with:\n",
        "* Multiple block variants (basic, residual, dense)\n",
        "* Robust search strategy with epsilon threshold\n",
        "* Enhanced training callbacks (early stopping, learning rate reduction)\n",
        "* Better regularization techniques\n",
        "* More efficient exploration algorithm"
      ],
      "metadata": {
        "id": "S7pJZhyyHImS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminaries"
      ],
      "metadata": {
        "id": "JoivgyMxAx73"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R8PtDnQ_Dat"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-model-optimization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AndreaMattiaGaravagno/ColabNAS"
      ],
      "metadata": {
        "id": "GtcExEHAhXZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x /content/ColabNAS/stm32tflm\n",
        "mv /content/ColabNAS/stm32tflm /content/"
      ],
      "metadata": {
        "id": "ImbM8NNWARqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improved ColabNAS Implementation"
      ],
      "metadata": {
        "id": "7EWcc1S1BVuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_model_optimization.python.core.keras.compat import keras\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import datetime\n",
        "import shutil\n",
        "import glob\n",
        "import re\n",
        "import os\n",
        "\n",
        "class ImprovedColabNAS:\n",
        "    architecture_name = 'resulting_architecture'\n",
        "    \n",
        "    def __init__(self, max_RAM, max_Flash, max_MACC, path_to_training_set, val_split, \n",
        "                 cache=False, input_shape=(50,50,3), save_path='.', \n",
        "                 path_to_stm32tflm='/content/stm32tflm'):\n",
        "        self.learning_rate = 1e-3\n",
        "        self.batch_size = 128\n",
        "        self.epochs = 100\n",
        "        \n",
        "        self.max_MACC = max_MACC\n",
        "        self.max_Flash = max_Flash\n",
        "        self.max_RAM = max_RAM\n",
        "        self.path_to_training_set = path_to_training_set\n",
        "        self.num_classes = len(next(os.walk(path_to_training_set))[1])\n",
        "        self.val_split = val_split\n",
        "        self.cache = cache\n",
        "        self.input_shape = input_shape\n",
        "        self.save_path = Path(save_path)\n",
        "        self.current_block_type = 'basic'\n",
        "        \n",
        "        self.path_to_trained_models = self.save_path / \"trained_models\"\n",
        "        self.path_to_trained_models.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        self.path_to_stm32tflm = Path(path_to_stm32tflm)\n",
        "        \n",
        "        self.load_training_set()\n",
        "    \n",
        "    def Model(self, k, c):\n",
        "        \"\"\"Enhanced model with block variants and regularization\"\"\"\n",
        "        kernel_size = (3,3)\n",
        "        pool_size = (2,2)\n",
        "        pool_strides = (2,2)\n",
        "        \n",
        "        number_of_cells_limited = False\n",
        "        number_of_mac = 0\n",
        "        \n",
        "        inputs = keras.Input(shape=self.input_shape)\n",
        "        \n",
        "        # Convolutional base\n",
        "        n = int(k)\n",
        "        multiplier = 2\n",
        "        \n",
        "        # First convolutional layer\n",
        "        c_in = self.input_shape[2]\n",
        "        x = keras.layers.Conv2D(n, kernel_size, padding='same')(inputs)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.ReLU()(x)\n",
        "        \n",
        "        number_of_mac += (c_in * kernel_size[0] * kernel_size[1] * x.shape[1] * x.shape[2] * x.shape[3])\n",
        "        \n",
        "        # Adding cells with block variants\n",
        "        for i in range(1, c + 1):\n",
        "            if x.shape[1] <= 1 or x.shape[2] <= 1:\n",
        "                number_of_cells_limited = True\n",
        "                break\n",
        "                \n",
        "            n = int(np.ceil(n * multiplier))\n",
        "            multiplier = multiplier - 2**-i\n",
        "            \n",
        "            # Apply pooling\n",
        "            x = keras.layers.MaxPooling2D(pool_size=pool_size, strides=pool_strides, padding='valid')(x)\n",
        "            c_in = x.shape[3]\n",
        "            \n",
        "            # Apply block variant\n",
        "            if self.current_block_type == 'residual' and c_in == n:\n",
        "                # Residual block\n",
        "                residual = x\n",
        "                x = keras.layers.Conv2D(n, kernel_size, padding='same')(x)\n",
        "                x = keras.layers.BatchNormalization()(x)\n",
        "                x = keras.layers.ReLU()(x)\n",
        "                x = keras.layers.Add()([x, residual])\n",
        "            elif self.current_block_type == 'dense':\n",
        "                # Dense block (concatenation)\n",
        "                x1 = keras.layers.Conv2D(n//2, kernel_size, padding='same')(x)\n",
        "                x1 = keras.layers.BatchNormalization()(x1)\n",
        "                x1 = keras.layers.ReLU()(x1)\n",
        "                x = keras.layers.Concatenate()([x, x1])\n",
        "                x = keras.layers.Conv2D(n, (1,1), padding='same')(x)  # 1x1 conv to adjust channels\n",
        "                x = keras.layers.BatchNormalization()(x)\n",
        "                x = keras.layers.ReLU()(x)\n",
        "            else:\n",
        "                # Basic block\n",
        "                x = keras.layers.Conv2D(n, kernel_size, padding='same')(x)\n",
        "                x = keras.layers.BatchNormalization()(x)\n",
        "                x = keras.layers.ReLU()(x)\n",
        "            \n",
        "            number_of_mac += (c_in * kernel_size[0] * kernel_size[1] * x.shape[1] * x.shape[2] * x.shape[3])\n",
        "        \n",
        "        # Classifier with dropout for better generalization\n",
        "        x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "        input_shape = x.shape[1]\n",
        "        x = keras.layers.Dropout(0.2)(x)\n",
        "        x = keras.layers.Dense(n)(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.ReLU()(x)\n",
        "        number_of_mac += (input_shape * x.shape[1])\n",
        "        x = keras.layers.Dropout(0.1)(x)\n",
        "        x = keras.layers.Dense(self.num_classes)(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        outputs = keras.layers.Softmax()(x)\n",
        "        number_of_mac += (x.shape[1] * outputs.shape[1])\n",
        "        \n",
        "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "        \n",
        "        # Use adaptive learning rate with decay\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=self.learning_rate, decay=1e-6)\n",
        "        model.compile(optimizer=opt,\n",
        "                     loss='categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "        \n",
        "        model.summary()\n",
        "        \n",
        "        return model, number_of_mac, number_of_cells_limited\n",
        "    \n",
        "    def load_training_set(self):\n",
        "        \"\"\"Load and preprocess training data\"\"\"\n",
        "        if 3 == self.input_shape[2]:\n",
        "            color_mode = 'rgb'\n",
        "        elif 1 == self.input_shape[2]:\n",
        "            color_mode = 'grayscale'\n",
        "        \n",
        "        train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "            directory=self.path_to_training_set,\n",
        "            labels='inferred',\n",
        "            label_mode='categorical',\n",
        "            color_mode=color_mode,\n",
        "            batch_size=self.batch_size,\n",
        "            image_size=self.input_shape[0:2],\n",
        "            shuffle=True,\n",
        "            seed=11,\n",
        "            validation_split=self.val_split,\n",
        "            subset='training'\n",
        "        )\n",
        "        \n",
        "        validation_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "            directory=self.path_to_training_set,\n",
        "            labels='inferred',\n",
        "            label_mode='categorical',\n",
        "            color_mode=color_mode,\n",
        "            batch_size=self.batch_size,\n",
        "            image_size=self.input_shape[0:2],\n",
        "            shuffle=True,\n",
        "            seed=11,\n",
        "            validation_split=self.val_split,\n",
        "            subset='validation'\n",
        "        )\n",
        "        \n",
        "        # Enhanced data augmentation\n",
        "        data_augmentation = tf.keras.Sequential([\n",
        "            tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "            tf.keras.layers.RandomRotation(0.2, fill_mode='constant', interpolation='bilinear'),\n",
        "            tf.keras.layers.RandomZoom(0.1),\n",
        "            tf.keras.layers.RandomContrast(0.1)\n",
        "        ])\n",
        "        \n",
        "        if self.cache:\n",
        "            self.train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y), \n",
        "                                       num_parallel_calls=tf.data.AUTOTUNE).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "            self.validation_ds = validation_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        else:\n",
        "            self.train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y), \n",
        "                                       num_parallel_calls=tf.data.AUTOTUNE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "            self.validation_ds = validation_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    \n",
        "    def quantize_model_uint8(self):\n",
        "        \"\"\"Quantize model to uint8\"\"\"\n",
        "        def representative_dataset():\n",
        "            for data in self.train_ds.rebatch(1).take(150):\n",
        "                yield [tf.dtypes.cast(data[0], tf.float32)]\n",
        "        \n",
        "        model = tf.keras.models.load_model(self.path_to_trained_models / f\"{self.model_name}.h5\")\n",
        "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "        converter.representative_dataset = representative_dataset\n",
        "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "        converter.inference_input_type = tf.uint8\n",
        "        converter.inference_output_type = tf.uint8\n",
        "        tflite_quant_model = converter.convert()\n",
        "        \n",
        "        with open(self.path_to_trained_models / f\"{self.model_name}.tflite\", 'wb') as f:\n",
        "            f.write(tflite_quant_model)\n",
        "        \n",
        "        (self.path_to_trained_models / f\"{self.model_name}.h5\").unlink()\n",
        "    \n",
        "    def evaluate_flash_and_peak_RAM_occupancy(self):\n",
        "        \"\"\"Evaluate Flash and RAM occupancy using STM32 tools\"\"\"\n",
        "        self.quantize_model_uint8()\n",
        "        \n",
        "        proc = subprocess.Popen([self.path_to_stm32tflm, self.path_to_trained_models / f\"{self.model_name}.tflite\"], \n",
        "                               stdout=subprocess.PIPE)\n",
        "        try:\n",
        "            outs, errs = proc.communicate(timeout=15)\n",
        "            Flash, RAM = re.findall(r'\\d+', str(outs))\n",
        "        except subprocess.TimeoutExpired:\n",
        "            proc.kill()\n",
        "            outs, errs = proc.communicate()\n",
        "            print(\"stm32tflm error\")\n",
        "            exit()\n",
        "        \n",
        "        return int(Flash), int(RAM)\n",
        "    \n",
        "    def evaluate_model_process(self, k, c):\n",
        "        \"\"\"Enhanced model evaluation with better callbacks\"\"\"\n",
        "        if k > 0:\n",
        "            self.model_name = f\"k_{k}_c_{c}_{self.current_block_type}\"\n",
        "            print(f\"\\n{self.model_name}\\n\")\n",
        "            \n",
        "            # Enhanced callbacks\n",
        "            checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "                str(self.path_to_trained_models / f\"{self.model_name}.h5\"), \n",
        "                monitor='val_accuracy',\n",
        "                verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
        "            \n",
        "            early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_accuracy', patience=15, restore_best_weights=True)\n",
        "            \n",
        "            reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss', factor=0.5, patience=8, min_lr=1e-7)\n",
        "            \n",
        "            model, MACC, number_of_cells_limited = self.Model(k, c)\n",
        "            \n",
        "            # Initial training for resource evaluation\n",
        "            model.fit(self.train_ds, epochs=1, validation_data=self.validation_ds, validation_freq=1)\n",
        "            model.save(self.path_to_trained_models / f\"{self.model_name}.h5\")\n",
        "            Flash, RAM = self.evaluate_flash_and_peak_RAM_occupancy()\n",
        "            print(f\"\\nRAM: {RAM},\\t Flash: {Flash},\\t MACC: {MACC}\\n\")\n",
        "            \n",
        "            if MACC <= self.max_MACC and Flash <= self.max_Flash and RAM <= self.max_RAM and not number_of_cells_limited:\n",
        "                hist = model.fit(self.train_ds, epochs=self.epochs - 1, validation_data=self.validation_ds, \n",
        "                               validation_freq=1, callbacks=[checkpoint, early_stopping, reduce_lr])\n",
        "                self.quantize_model_uint8()\n",
        "                \n",
        "            return {'k': k,\n",
        "                    'c': c if not number_of_cells_limited else \"Not feasible\",\n",
        "                    'RAM': RAM if RAM <= self.max_RAM else \"Outside the upper bound\",\n",
        "                    'Flash': Flash if Flash <= self.max_Flash else \"Outside the upper bound\",\n",
        "                    'MACC': MACC if MACC <= self.max_MACC else \"Outside the upper bound\",\n",
        "                    'max_val_acc': np.around(np.amax(hist.history['val_accuracy']), decimals=3)\n",
        "                    if 'hist' in locals() else -3}\n",
        "        else:\n",
        "            return {'k': 'unfeasible', 'c': c, 'max_val_acc': -3}\n",
        "    \n",
        "    def explore_num_cells(self, k):\n",
        "        \"\"\"Robust exploration with epsilon threshold\"\"\"\n",
        "        previous_architecture = {'k': -1, 'c': -1, 'max_val_acc': -2}\n",
        "        current_architecture = {'k': -1, 'c': -1, 'max_val_acc': -1}\n",
        "        c = -1\n",
        "        k = int(k)\n",
        "        epsilon = 0.005\n",
        "        \n",
        "        # More robust exploration with epsilon threshold\n",
        "        while current_architecture['max_val_acc'] > previous_architecture['max_val_acc'] + epsilon:\n",
        "            previous_architecture = current_architecture\n",
        "            c = c + 1\n",
        "            self.model_counter = self.model_counter + 1\n",
        "            current_architecture = self.evaluate_model_process(k, c)\n",
        "            print(f\"\\n\\n\\n{current_architecture}\\n\\n\\n\")\n",
        "            \n",
        "            # Early termination if constraints are violated\n",
        "            if (current_architecture['k'] == 'unfeasible' or \n",
        "                current_architecture['c'] == \"Not feasible\" or\n",
        "                current_architecture['RAM'] == \"Outside the upper bound\" or\n",
        "                current_architecture['Flash'] == \"Outside the upper bound\" or\n",
        "                current_architecture['MACC'] == \"Outside the upper bound\"):\n",
        "                break\n",
        "                \n",
        "        return previous_architecture\n",
        "    \n",
        "    def search(self):\n",
        "        \"\"\"Enhanced search with block variants\"\"\"\n",
        "        self.model_counter = 0\n",
        "        epsilon = 0.005\n",
        "        k0 = 4\n",
        "        \n",
        "        # Block variants to explore different architectures\n",
        "        block_variants = ['basic', 'residual', 'dense']\n",
        "        \n",
        "        start = datetime.datetime.now()\n",
        "        \n",
        "        best_architecture = {'k': -1, 'c': -1, 'block': 'basic', 'max_val_acc': -1}\n",
        "        \n",
        "        # Explore each block variant\n",
        "        for block_type in block_variants:\n",
        "            print(f\"\\nExploring block variant: {block_type}\")\n",
        "            self.current_block_type = block_type\n",
        "            \n",
        "            k = k0\n",
        "            previous_architecture = self.explore_num_cells(k)\n",
        "            previous_architecture['block'] = block_type\n",
        "            \n",
        "            k = 2 * k\n",
        "            current_architecture = self.explore_num_cells(k)\n",
        "            current_architecture['block'] = block_type\n",
        "\n",
        "            if current_architecture['max_val_acc'] > previous_architecture['max_val_acc'] + epsilon:\n",
        "                # Expanding search - keep doubling k while improvement continues\n",
        "                while current_architecture['max_val_acc'] > previous_architecture['max_val_acc'] + epsilon:\n",
        "                    previous_architecture = current_architecture\n",
        "                    k = 2 * k\n",
        "                    current_architecture = self.explore_num_cells(k)\n",
        "                    current_architecture['block'] = block_type\n",
        "                    if current_architecture['k'] == 'unfeasible':\n",
        "                        break\n",
        "            else:\n",
        "                # Contracting search - keep halving k while improvement continues\n",
        "                k = k0 / 2\n",
        "                current_architecture = self.explore_num_cells(k)\n",
        "                current_architecture['block'] = block_type\n",
        "                while (current_architecture['max_val_acc'] >= previous_architecture['max_val_acc'] and k >= 1):\n",
        "                    previous_architecture = current_architecture\n",
        "                    k = k / 2\n",
        "                    current_architecture = self.explore_num_cells(k)\n",
        "                    current_architecture['block'] = block_type\n",
        "                    if current_architecture['k'] == 'unfeasible' or k < 1:\n",
        "                        break\n",
        "\n",
        "            # Update best architecture if current is better\n",
        "            if previous_architecture['max_val_acc'] > best_architecture['max_val_acc']:\n",
        "                best_architecture = previous_architecture\n",
        "                \n",
        "        resulting_architecture = best_architecture\n",
        "\n",
        "        end = datetime.datetime.now()\n",
        "\n",
        "        if resulting_architecture['max_val_acc'] > 0:\n",
        "            resulting_architecture_name = f\"k_{resulting_architecture['k']}_c_{resulting_architecture['c']}_{resulting_architecture['block']}.tflite\"\n",
        "            self.path_to_resulting_architecture = self.save_path / f\"resulting_architecture_{resulting_architecture_name}\"\n",
        "            (self.path_to_trained_models / f\"k_{resulting_architecture['k']}_c_{resulting_architecture['c']}_{resulting_architecture['block']}.tflite\").rename(self.path_to_resulting_architecture)\n",
        "            shutil.rmtree(self.path_to_trained_models)\n",
        "            print(f\"\\nResulting architecture: {resulting_architecture}\\n\")\n",
        "        else:\n",
        "            print(f\"\\nNo feasible architecture found\\n\")\n",
        "        print(f\"Elapsed time (search): {end-start}\\n\")\n",
        "\n",
        "        return self.path_to_resulting_architecture if resulting_architecture['max_val_acc'] > 0 else None"
      ],
      "metadata": {
        "id": "9r2J-EoGBdYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try Improved ColabNAS!"
      ],
      "metadata": {
        "id": "z_qOEXpzBw6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "data_dir = tf.keras.utils.get_file('flower_photos.tar', origin=dataset_url, extract=True)\n",
        "data_dir = Path(data_dir).with_suffix('')"
      ],
      "metadata": {
        "id": "POrh-7WTDtkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "input_shape = (50,50,3)\n",
        "\n",
        "# Target: STM32L412KBU3\n",
        "# 273 CoreMark, 40 kiB RAM, 128 kiB Flash\n",
        "peak_RAM_upper_bound = 40960\n",
        "Flash_upper_bound = 131072\n",
        "MACC_upper_bound = 2730000  # CoreMark * 1e4\n",
        "\n",
        "path_to_training_set = data_dir\n",
        "val_split = 0.3\n",
        "cache = True\n",
        "save_path = '/content/'\n",
        "\n",
        "# Show GPU info\n",
        "!nvidia-smi\n",
        "\n",
        "# Initialize improved ColabNAS\n",
        "improved_nas = ImprovedColabNAS(\n",
        "    peak_RAM_upper_bound, Flash_upper_bound, MACC_upper_bound, \n",
        "    path_to_training_set, val_split, cache, input_shape, save_path=save_path\n",
        ")\n",
        "\n",
        "# Run enhanced search\n",
        "path_to_tflite_model = improved_nas.search()"
      ],
      "metadata": {
        "id": "mRKXWAXqBy7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Improvements\n",
        "\n",
        "### 1. Block Variants\n",
        "- **Basic Block**: Standard convolution + batch norm + ReLU\n",
        "- **Residual Block**: Adds skip connections for better gradient flow\n",
        "- **Dense Block**: Concatenates features for better information flow\n",
        "\n",
        "### 2. Enhanced Search Strategy\n",
        "- Epsilon threshold (0.005) for more robust convergence\n",
        "- Early termination when constraints are violated\n",
        "- Systematic exploration of multiple block types\n",
        "\n",
        "### 3. Better Training\n",
        "- Early stopping to prevent overfitting\n",
        "- Learning rate reduction on plateau\n",
        "- Enhanced data augmentation (zoom, contrast)\n",
        "- Dropout layers for regularization\n",
        "\n",
        "### 4. Improved Robustness\n",
        "- Better error handling\n",
        "- More comprehensive constraint checking\n",
        "- Adaptive learning rate with decay\n",
        "\n",
        "This implementation follows the robust search strategy from the pseudo code while adding practical improvements for better performance and reliability."
      ],
      "metadata": {
        "id": "improvement_summary"
      }
    }
  ]
}