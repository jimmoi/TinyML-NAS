{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local ColabNAS - Neural Architecture Search for TinyML\n",
    "\n",
    "This notebook demonstrates how to use ColabNAS locally on your machine for finding optimal CNN architectures for TinyML devices.\n",
    "\n",
    "## Key Features:\n",
    "- Runs locally without Google Colab dependencies\n",
    "- Optimized for resource-constrained devices\n",
    "- Automatic quantization to uint8\n",
    "- Memory usage estimation\n",
    "- Early stopping for faster convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "GPU available: []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from local_colabnas import LocalColabNAS, test_tflite_model\n",
    "\n",
    "# Check TensorFlow version and GPU availability\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Set memory growth for GPU (if available)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download and Prepare Dataset\n",
    "\n",
    "We'll use the flower dataset as an example. You can replace this with your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to: C:\\Users\\Chakkaphan\\.keras\\datasets\\flower_photos_extracted\n",
      "Classes found: ['flower_photos']\n"
     ]
    }
   ],
   "source": [
    "# Download flower dataset\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file('flower_photos.tar', origin=dataset_url, extract=True)\n",
    "data_dir = Path(data_dir).with_suffix('')\n",
    "\n",
    "print(f\"Dataset downloaded to: {data_dir}\")\n",
    "print(f\"Classes found: {[d.name for d in data_dir.iterdir() if d.is_dir()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Target Device Constraints\n",
    "\n",
    "Define the hardware constraints for your target TinyML device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target constraints:\n",
      "  RAM: 40.0 KB\n",
      "  Flash: 128.0 KB\n",
      "  MACC: 2,730,000\n",
      "  Input shape: (64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# Input image configuration\n",
    "input_shape = (64, 64, 3)  # Slightly larger for better accuracy\n",
    "\n",
    "# Target device: STM32L412KBU3 (example)\n",
    "# 273 CoreMark, 40 kiB RAM, 128 kiB Flash\n",
    "peak_RAM_upper_bound = 40 * 1024      # 40 KB\n",
    "Flash_upper_bound = 128 * 1024        # 128 KB  \n",
    "MACC_upper_bound = 273 * 10000        # CoreMark * 1e4\n",
    "\n",
    "# Training configuration\n",
    "val_split = 0.2\n",
    "cache = False  # Set to True if you have enough RAM\n",
    "save_path = Path('./nas_results')\n",
    "save_path.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Target constraints:\")\n",
    "print(f\"  RAM: {peak_RAM_upper_bound/1024:.1f} KB\")\n",
    "print(f\"  Flash: {Flash_upper_bound/1024:.1f} KB\")\n",
    "print(f\"  MACC: {MACC_upper_bound:,}\")\n",
    "print(f\"  Input shape: {input_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize LocalColabNAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LocalColabNAS with 1 classes\n",
      "Found 3670 files belonging to 1 classes.\n",
      "Using 2936 files for training.\n",
      "Found 3670 files belonging to 1 classes.\n",
      "Using 734 files for validation.\n",
      "LocalColabNAS initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the NAS system\n",
    "nas = LocalColabNAS(\n",
    "    max_RAM=peak_RAM_upper_bound,\n",
    "    max_Flash=Flash_upper_bound, \n",
    "    max_MACC=MACC_upper_bound,\n",
    "    path_to_training_set=str(data_dir),\n",
    "    val_split=val_split,\n",
    "    cache=cache,\n",
    "    input_shape=input_shape,\n",
    "    save_path=str(save_path)\n",
    ")\n",
    "\n",
    "print(\"LocalColabNAS initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Neural Architecture Search\n",
    "\n",
    "This will automatically search for the best architecture within your constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Neural Architecture Search...\n",
      "This may take several minutes to hours depending on your hardware.\n",
      "Starting NAS search at 2026-01-05 16:36:24.386121\n",
      "\n",
      "k_4_c_0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chakkaphan\\anaconda3\\envs\\tinyml\\lib\\site-packages\\keras\\src\\ops\\nn.py:944: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n",
      "c:\\Users\\Chakkaphan\\anaconda3\\envs\\tinyml\\lib\\site-packages\\keras\\src\\losses\\losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAM: 49844, Flash: 57792, MACC: 442385\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'k': 4, 'c': 0, 'RAM': 'Outside the upper bound', 'Flash': 57792, 'MACC': 442385, 'max_val_acc': -3}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "k_8_c_0\n",
      "\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAM: 50644, Flash: 62008, MACC: 884801\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'k': 8, 'c': 0, 'RAM': 'Outside the upper bound', 'Flash': 62008, 'MACC': 884801, 'max_val_acc': -3}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "k_2_c_0\n",
      "\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAM: 49492, Flash: 57912, MACC: 221189\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'k': 2, 'c': 0, 'RAM': 'Outside the upper bound', 'Flash': 57912, 'MACC': 221189, 'max_val_acc': -3}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "k_1_c_0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start the search process\n",
    "print(\"Starting Neural Architecture Search...\")\n",
    "print(\"This may take several minutes to hours depending on your hardware.\")\n",
    "\n",
    "best_model_path = nas.search()\n",
    "\n",
    "if best_model_path:\n",
    "    print(f\"\\nâœ… Search completed successfully!\")\n",
    "    print(f\"Best model saved at: {best_model_path}\")\n",
    "else:\n",
    "    print(\"\\nâŒ No feasible architecture found within the given constraints.\")\n",
    "    print(\"Consider relaxing the constraints or using a smaller input size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test the Found Architecture\n",
    "\n",
    "Let's test the performance of the discovered model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_path and best_model_path.exists():\n",
    "    # Create a test dataset\n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=str(data_dir),\n",
    "        labels='inferred',\n",
    "        label_mode='categorical',\n",
    "        color_mode='rgb' if input_shape[2] == 3 else 'grayscale',\n",
    "        batch_size=32,\n",
    "        image_size=input_shape[0:2],\n",
    "        shuffle=True,\n",
    "        seed=42,\n",
    "        validation_split=0.8,  # Use 20% for testing\n",
    "        subset='validation'\n",
    "    )\n",
    "    \n",
    "    # Test the model\n",
    "    print(\"Testing the discovered model...\")\n",
    "    accuracy = test_tflite_model(best_model_path, test_ds)\n",
    "    \n",
    "    # Get model size\n",
    "    model_size = best_model_path.stat().st_size\n",
    "    print(f\"Model size: {model_size/1024:.2f} KB\")\n",
    "    print(f\"Model path: {best_model_path}\")\n",
    "else:\n",
    "    print(\"No model to test - search was unsuccessful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Manual Architecture Testing (Optional)\n",
    "\n",
    "You can also test specific architectures manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a specific architecture manually\n",
    "def test_manual_architecture(k, c):\n",
    "    \"\"\"Test a specific architecture configuration\"\"\"\n",
    "    print(f\"Testing architecture: k={k}, c={c}\")\n",
    "    \n",
    "    model, macc, cells_limited = nas.Model(k, c)\n",
    "    model.summary()\n",
    "    \n",
    "    print(f\"MACC: {macc:,}\")\n",
    "    print(f\"Cells limited: {cells_limited}\")\n",
    "    print(f\"Total parameters: {model.count_params():,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example: test a small architecture\n",
    "# test_model = test_manual_architecture(k=4, c=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deployment Information\n",
    "\n",
    "Information about deploying your model to actual hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš€ Deployment Information:\")\n",
    "print(\"\\n1. Your quantized TFLite model is ready for deployment\")\n",
    "print(\"2. The model uses uint8 quantization for optimal performance\")\n",
    "print(\"3. For STM32 deployment, you can use STM32Cube.AI\")\n",
    "print(\"4. For other microcontrollers, use TensorFlow Lite Micro\")\n",
    "\n",
    "if best_model_path and best_model_path.exists():\n",
    "    print(f\"\\nðŸ“ Model file: {best_model_path}\")\n",
    "    print(f\"ðŸ“Š Model size: {best_model_path.stat().st_size/1024:.2f} KB\")\n",
    "    \n",
    "    # Load and inspect the model\n",
    "    interpreter = tf.lite.Interpreter(str(best_model_path))\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "    \n",
    "    print(f\"\\nðŸ”§ Model Details:\")\n",
    "    print(f\"  Input shape: {input_details['shape']}\")\n",
    "    print(f\"  Input type: {input_details['dtype']}\")\n",
    "    print(f\"  Output shape: {output_details['shape']}\")\n",
    "    print(f\"  Output type: {output_details['dtype']}\")\n",
    "    \n",
    "    if input_details['dtype'] == tf.uint8:\n",
    "        scale, zero_point = input_details['quantization']\n",
    "        print(f\"  Input quantization - Scale: {scale:.6f}, Zero point: {zero_point}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization (Optional)\n",
    "\n",
    "Visualize some sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model_path, dataset, num_samples=6):\n",
    "    \"\"\"Visualize model predictions on sample images\"\"\"\n",
    "    if not model_path or not model_path.exists():\n",
    "        print(\"No model available for visualization\")\n",
    "        return\n",
    "    \n",
    "    interpreter = tf.lite.Interpreter(str(model_path))\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "    \n",
    "    # Get class names\n",
    "    class_names = [d.name for d in data_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    sample_count = 0\n",
    "    for batch in dataset.take(1):\n",
    "        images, labels = batch\n",
    "        \n",
    "        for i in range(min(num_samples, images.shape[0])):\n",
    "            image = images[i:i+1]\n",
    "            true_label = labels[i].numpy().argmax()\n",
    "            \n",
    "            # Prepare input\n",
    "            if input_details['dtype'] == tf.uint8:\n",
    "                scale, zero_point = input_details['quantization']\n",
    "                image_input = image / scale + zero_point\n",
    "                image_input = tf.cast(image_input, tf.uint8)\n",
    "            else:\n",
    "                image_input = tf.cast(image, input_details['dtype'])\n",
    "            \n",
    "            # Make prediction\n",
    "            interpreter.set_tensor(input_details['index'], image_input)\n",
    "            interpreter.invoke()\n",
    "            prediction = interpreter.get_tensor(output_details['index'])\n",
    "            predicted_label = prediction.argmax()\n",
    "            confidence = prediction.max()\n",
    "            \n",
    "            # Plot\n",
    "            plt.subplot(2, 3, i + 1)\n",
    "            plt.imshow(image[0].numpy().astype(np.uint8))\n",
    "            plt.title(f'True: {class_names[true_label]}\\n'\n",
    "                     f'Pred: {class_names[predicted_label]}\\n'\n",
    "                     f'Conf: {confidence:.2f}')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            sample_count += 1\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "        \n",
    "        if sample_count >= num_samples:\n",
    "            break\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions if model exists\n",
    "if best_model_path and best_model_path.exists():\n",
    "    visualize_predictions(best_model_path, test_ds)\n",
    "else:\n",
    "    print(\"No model available for visualization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
